#!/usr/bin/env python3
# server.py (Cursor Agent-Mode Compatible + Tool Prompt Examples)

import os
import json
from typing import List, Dict
from datetime import datetime

from fastapi import FastAPI
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from pymongo import MongoClient

MODELS = {
    "deepseek": "deepseek-ai/deepseek-coder-7b-instruct-v1.5"
}

class ChatReq(BaseModel):
    model: str
    messages: List[Dict[str, str]]
    temperature: float = 1.0
    max_tokens: int = 16_384
    stream: bool = False
    session_id: str = "default"

client = MongoClient("mongodb://localhost:27017")
db = client.agent_memory
messages_col = db.messages

def extract_tool_call(text: str):
    try:
        start = text.index('{')
        end = text.rindex('}') + 1
        return json.loads(text[start:end])
    except Exception:
        return {"tool": "NONE", "args": {}}

def log_message(session_id: str, role: str, content: str):
    messages_col.insert_one({
        "session_id": session_id,
        "role": role,
        "content": content,
        "timestamp": datetime.utcnow()
    })

def create_app():
    model_id = MODELS[os.getenv("MODEL_NAME", "deepseek")]
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map={"": 0}
    )

    app = FastAPI()
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_methods=["*"],
        allow_headers=["*"]
    )

    async def chat(req: ChatReq):
        prompt = """You are a developer assistant that only responds with a valid JSON object.

You can use one of the following tools: EDIT, TERMINAL, FILE, EXPLAIN.

Examples:

Create a new file:
{
  "tool": "FILE",
  "args": {
    "action": "write",
    "filename": "hello.txt",
    "content": "Hello, world!"
  }
}

Run a command:
{
  "tool": "TERMINAL",
  "args": {
    "command": "npm install"
  }
}

Modify a file:
{
  "tool": "EDIT",
  "args": {
    "filename": "main.py",
    "patch": [
      { "op": "replace", "line": 3, "content": "print('Updated')" }
    ]
  }
}

Only respond with the JSON tool object, nothing else.
"""

        for m in req.messages:
            role = m.get("role", "user").capitalize()
            prompt += f"{role}: {m.get('content', '')}\n"
        prompt += "Assistant:"

        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        if inputs.input_ids.shape[1] > model.config.max_position_embeddings:
            for k in inputs:
                inputs[k] = inputs[k][:, -model.config.max_position_embeddings:]

        with torch.no_grad():
            output = model.generate(
                **inputs,
                max_new_tokens=req.max_tokens,
                do_sample=req.temperature > 0,
                temperature=req.temperature or 1.0
            )

        new_tokens = output[0][inputs["input_ids"].shape[1]:]
        text = tokenizer.decode(new_tokens, skip_special_tokens=True)
        log_message(req.session_id, "assistant", text)

        tool = extract_tool_call(text)
        result = tool if tool["tool"] != "NONE" else {"tool": "EXPLAIN", "args": {"explanation": text}}

        if req.stream:
            def event_stream():
                chunk = {
                    "choices": [{
                        "delta": {"content": json.dumps(result)},
                        "index": 0,
                        "finish_reason": "stop"
                    }]
                }
                yield f"data: {json.dumps(chunk)}\n\n"
                yield "data: [DONE]\n\n"
            return StreamingResponse(event_stream(), media_type="text/event-stream")

        return {
            "id": "local-1",
            "object": "chat.completion",
            "choices": [{
                "index": 0,
                "message": {"role": "assistant", "content": json.dumps(result)},
                "finish_reason": "stop"
            }],
            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
        }

    @app.post("/v1/chat/completions")
    async def chat_v1(req: ChatReq):
        return await chat(req)

    @app.post("/chat/completions")
    async def chat_root(req: ChatReq):
        return await chat(req)

    @app.get("/models")
    async def list_models():
        return {
            "object": "list",
            "data": [{"id": k, "object": "model", "owned_by": "local"} for k in MODELS]
        }

    @app.get("/v1/models")
    async def list_models_v1():
        return await list_models()

    return app

app = create_app()
